{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cap11_2_cnn_alexnet.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNk0DQ5dWLpsylW920OMTyR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrpb/cic0269_natural_language_processing/blob/main/lectures/cap11_2_cnn_alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tb5JozJiFDBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1dad2e2-2eee-4b83-9805-40f3d3a9cec4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "6dfyHBHHhQqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b5e204-3324-4c21-8c5d-1cb4dbe100c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Collecting keras\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires keras<2.9,>=2.8.0rc0, but you have keras 2.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "Successfully installed keras-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/drive/MyDrive/ocr/full_3/' 'documents'"
      ],
      "metadata": {
        "id": "SbzoQKYgFj18"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Conv2D,MaxPooling2D,BatchNormalization,Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "y6JFUcrWhSaB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leitura de dados"
      ],
      "metadata": {
        "id": "mMlOC9NtJQNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_documents = 'documents'\n",
        "lista_subfolders = os.listdir(path_documents)\n",
        "\n",
        "dataset_dict = {}\n",
        "\n",
        "dataset_dict['filename'] = []\n",
        "dataset_dict['label'] = []\n",
        "\n",
        "for folder in lista_subfolders:\n",
        "    lista_imagens = os.listdir(path_documents+'/'+folder)\n",
        "\n",
        "    for img_file in lista_imagens:\n",
        "        dataset_dict['filename'].append(folder+'/'+img_file)\n",
        "        dataset_dict['label'].append(folder)"
      ],
      "metadata": {
        "id": "eITuq3OnKdEq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataset_dict,columns=['filename','label'])"
      ],
      "metadata": {
        "id": "McRygJeDNIlQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['filename'].values\n",
        "y = df['label'].values"
      ],
      "metadata": {
        "id": "q2vXYvZJKxeB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.6, random_state=42)"
      ],
      "metadata": {
        "id": "ku6mg89eJms9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação dos logits dos labels"
      ],
      "metadata": {
        "id": "NmXVHUj6OsbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "train_labels = pd.Categorical(y_train)\n",
        "valid_labels = pd.Categorical(y_valid)\n",
        "test_labels = pd.Categorical(y_test)\n",
        "\n",
        "y_train_int = train_labels.codes\n",
        "y_valid_int = valid_labels.codes\n",
        "y_test_int = test_labels.codes\n",
        "\n",
        "y_train_logits = to_categorical(y_train_int)\n",
        "y_valid_logits = to_categorical(y_valid_int)\n",
        "y_test_logits = to_categorical(y_test_int)"
      ],
      "metadata": {
        "id": "8sPRtB2oOujl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista_train = []\n",
        "for i in range(0,len(X_train)):\n",
        "    lista_train.append([X_train[i],y_train[i]])\n",
        "\n",
        "lista_valid = []\n",
        "for i in range(0,len(X_valid)):\n",
        "    lista_valid.append([X_valid[i],y_valid[i]])\n",
        "\n",
        "lista_test = []\n",
        "for i in range(0,len(X_test)):\n",
        "    lista_test.append([X_test[i],y_test[i]])"
      ],
      "metadata": {
        "id": "TkbKFucdQ7C2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.DataFrame(lista_train,columns = ['filename','label'])\n",
        "df_valid = pd.DataFrame(lista_valid,columns = ['filename','label'])\n",
        "df_test = pd.DataFrame(lista_test,columns = ['filename','label'])"
      ],
      "metadata": {
        "id": "agY1B8lPRt0j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = datagen.flow_from_dataframe(df_train,\n",
        "                                              directory=path_documents,\n",
        "                                              x_col='filename',y_col='label',\n",
        "                                              class_mode='categorical',\n",
        "                                              shuffle=True)\n",
        "\n",
        "valid_generator = datagen.flow_from_dataframe(df_valid,\n",
        "                                              directory=path_documents,\n",
        "                                              x_col='filename',y_col='label',\n",
        "                                              class_mode='categorical',\n",
        "                                              shuffle=True)\n",
        "\n",
        "test_generator = datagen.flow_from_dataframe(df_test,\n",
        "                                              directory=path_documents,\n",
        "                                              x_col='filename',y_col='label',\n",
        "                                             class_mode='categorical',\n",
        "                                              shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9YGsrzzJRrr",
        "outputId": "e32df39e-b183-49e7-f944-93abadf8b8a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2072 validated image filenames belonging to 3 classes.\n",
            "Found 355 validated image filenames belonging to 3 classes.\n",
            "Found 534 validated image filenames belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.image_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eE9Wb8aYRgW",
        "outputId": "75ce4b7f-c102-451c-fc97-be97f6a6d759"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "A9i8TjKRjHJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dims = (224,224,3)\n",
        "num_classes = 3"
      ],
      "metadata": {
        "id": "vwZ2ynTPjHRI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# Parte 1 da AlexNet\n",
        "model.add(layers.experimental.preprocessing.Resizing(224,224,interpolation=\"bilinear\",input_shape=(224,224,3)))\n",
        "model.add(Conv2D(96,(11,11), strides=(4,4)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((3, 3),strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Parte 2 da AlexNet\n",
        "model.add(Conv2D(256,(5,5), strides=(1,1),padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((3, 3),strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Parte 3 da AlexNet\n",
        "model.add(Conv2D(384,(3,3), strides=(1,1),padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Conv2D(384,(3,3), strides=(1,1),padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Conv2D(256,(3,3), strides=(1,1),padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(MaxPooling2D((3, 3),strides=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation(\"softmax\"))"
      ],
      "metadata": {
        "id": "ADggsOKeh1Lc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dPoAyxJqk7l",
        "outputId": "0ef36816-0fe6-41ee-f991-2267bcd7d220"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resizing (Resizing)         (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 54, 54, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 96)       384       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 26, 26, 256)       614656    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 26, 26, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 384)       885120    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 12, 12, 384)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 12, 12, 384)       1327488   \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 12, 12, 384)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 12, 12, 256)       884992    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 12, 12, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 5, 5, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6400)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              26218496  \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 12291     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 46,760,707\n",
            "Trainable params: 46,760,003\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGD(lr=0.0001)\n",
        "\n",
        "model.compile(optimizer=sgd,loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_generator,\n",
        "          epochs=10,\n",
        "          batch_size=16,\n",
        "          validation_data=valid_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UPcf4FSHnw6",
        "outputId": "f73e6e47-7e35-4a8d-9f92-acdec82b848d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "49/65 [=====================>........] - ETA: 1:07 - loss: 0.9547 - accuracy: 0.4782"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transferência de Aprendizado"
      ],
      "metadata": {
        "id": "l1i2IbgOZhJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2VWCbNKtZlOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}