{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cap07_1_probabilistic_language_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPy9gN2T4a1gFPhpkUDDCW6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrpb/cic0269_natural_language_processing/blob/main/cap07_1_probabilistic_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capítulo 7 - Modelos de Linguagem Probabilísticos\n",
        "\n",
        "Um modelo de linguagem calcula a probabilidade de obtermos uma sentença ou uma sequência de palavras.\n",
        "\n",
        "Neste capítulo, vamos abordar os modelos de linguagem probabilísticos baseados em N-gramas.\n",
        "\n",
        "Vamos considerar o pequeno *corpus* criado nas aulas anteriores:"
      ],
      "metadata": {
        "id": "cjSw4r5UWB5k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6PBYceZ9wHfa"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "corpus.append('Batatinha quando nasce esparrama pelo chão igual batatinha')\n",
        "corpus.append('A pior experiência da minha vida')\n",
        "corpus.append('Quero meu dinheiro de volta pois é meu e é meu')\n",
        "corpus.append('A experiência do dinheiro esparrama minha vida')\n",
        "corpus.append('Acai é a melhor coisa da vida')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Unigramas\n",
        "\n",
        "Vamos criar um Bag-of-Words (BoW) tradicional baseado em contagem para utilizarmos na construção de um modelo de linguagem unigrama.\n",
        "\n",
        "Ao mesmo tempo, vamos coletar a quantidade total de termos $n$ de todo o corpus:"
      ],
      "metadata": {
        "id": "V_hcW86m6Nw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram = {}\n",
        "n=0\n",
        "for i,doc in enumerate(corpus):\n",
        "    unigram['sentenca {}'.format(i+1)] = dict()\n",
        "    tokens = doc.split()\n",
        "    for word in tokens:\n",
        "        if word in unigram['sentenca {}'.format(i+1)]:\n",
        "            unigram['sentenca {}'.format(i+1)][word] += 1\n",
        "        else:\n",
        "            unigram['sentenca {}'.format(i+1)][word] = 1\n",
        "        n+=1\n",
        "\n",
        "unigram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_pFQ9Qe6PX-",
        "outputId": "8f7d3a64-48a4-4c06-975a-1350d9738e2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentenca 1': {'Batatinha': 1,\n",
              "  'batatinha': 1,\n",
              "  'chão': 1,\n",
              "  'esparrama': 1,\n",
              "  'igual': 1,\n",
              "  'nasce': 1,\n",
              "  'pelo': 1,\n",
              "  'quando': 1},\n",
              " 'sentenca 2': {'A': 1,\n",
              "  'da': 1,\n",
              "  'experiência': 1,\n",
              "  'minha': 1,\n",
              "  'pior': 1,\n",
              "  'vida': 1},\n",
              " 'sentenca 3': {'Quero': 1,\n",
              "  'de': 1,\n",
              "  'dinheiro': 1,\n",
              "  'e': 1,\n",
              "  'meu': 3,\n",
              "  'pois': 1,\n",
              "  'volta': 1,\n",
              "  'é': 2},\n",
              " 'sentenca 4': {'A': 1,\n",
              "  'dinheiro': 1,\n",
              "  'do': 1,\n",
              "  'esparrama': 1,\n",
              "  'experiência': 1,\n",
              "  'minha': 1,\n",
              "  'vida': 1},\n",
              " 'sentenca 5': {'Acai': 1,\n",
              "  'a': 1,\n",
              "  'coisa': 1,\n",
              "  'da': 1,\n",
              "  'melhor': 1,\n",
              "  'vida': 1,\n",
              "  'é': 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2. Bigramas\n",
        "\n",
        "De maneira similar, podemos calcular os bigramas. Para isso, seja uma sentença $W$ que em sua forma tokenizada é descrita como $\\{w_1, w_2, \\ldots, w_n\\}$.\n",
        "\n",
        "O Bag-of-Words de bigramas é construído pegando cada par consecutivo de palavras $(w_1,w_2), (w_2,w_3), \\ldots, (w_{n-1},w_n)$. Vamos construir um Bag-of-Words de bigrams para facilitar o entendimento.\n",
        "\n",
        "**Atenção:** não se trata de uma permutação entre todas as palavras do *corpus*!\n",
        "\n",
        "Vamos criar um dicionário em que cada bigrama é uma chave sob a forma de uma tupla:"
      ],
      "metadata": {
        "id": "uauaK0IHWhQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = {}\n",
        "\n",
        "for i,doc in enumerate(corpus):\n",
        "    bigram['sentenca {}'.format(i+1)] = dict()\n",
        "    tokens = doc.split()\n",
        "    for j in range(0,len(tokens)-1):\n",
        "        par = (tokens[j],tokens[j+1])\n",
        "        if par in bigram['sentenca {}'.format(i+1)]:\n",
        "            bigram['sentenca {}'.format(i+1)][par] += 1\n",
        "        else:\n",
        "            bigram['sentenca {}'.format(i+1)][par] = 1\n",
        "\n",
        "bigram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SzeFNa5AdVZ",
        "outputId": "5357d16b-1c83-4605-fd6b-aff6b686c717"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentenca 1': {('Batatinha', 'quando'): 1,\n",
              "  ('chão', 'igual'): 1,\n",
              "  ('esparrama', 'pelo'): 1,\n",
              "  ('igual', 'batatinha'): 1,\n",
              "  ('nasce', 'esparrama'): 1,\n",
              "  ('pelo', 'chão'): 1,\n",
              "  ('quando', 'nasce'): 1},\n",
              " 'sentenca 2': {('A', 'pior'): 1,\n",
              "  ('da', 'minha'): 1,\n",
              "  ('experiência', 'da'): 1,\n",
              "  ('minha', 'vida'): 1,\n",
              "  ('pior', 'experiência'): 1},\n",
              " 'sentenca 3': {('Quero', 'meu'): 1,\n",
              "  ('de', 'volta'): 1,\n",
              "  ('dinheiro', 'de'): 1,\n",
              "  ('e', 'é'): 1,\n",
              "  ('meu', 'dinheiro'): 1,\n",
              "  ('meu', 'e'): 1,\n",
              "  ('pois', 'é'): 1,\n",
              "  ('volta', 'pois'): 1,\n",
              "  ('é', 'meu'): 2},\n",
              " 'sentenca 4': {('A', 'experiência'): 1,\n",
              "  ('dinheiro', 'esparrama'): 1,\n",
              "  ('do', 'dinheiro'): 1,\n",
              "  ('esparrama', 'minha'): 1,\n",
              "  ('experiência', 'do'): 1,\n",
              "  ('minha', 'vida'): 1},\n",
              " 'sentenca 5': {('Acai', 'é'): 1,\n",
              "  ('a', 'melhor'): 1,\n",
              "  ('coisa', 'da'): 1,\n",
              "  ('da', 'vida'): 1,\n",
              "  ('melhor', 'coisa'): 1,\n",
              "  ('é', 'a'): 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3. Modelo de Linguagem baseado em Bigramas\n",
        "\n",
        "Queremos calcular a probabilidade de obtermos uma sentença $W$, isto é, $P(W) = P(w_1,w_2,\\ldots,w_n)$.\n",
        "\n",
        "Considerando que essas probabilidades são calculadas com base em um vocabulário \"gigante\" e representativo, desenvolvemos conforme a Regra da Cadeia:\n",
        "\n",
        "\\begin{equation}\n",
        "P(w_1,w_2,w_3,\\ldots,w_n) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1,w_2) \\times \\ldots P(w_n | w_1,w_2, \\ldots, w_{n-1})\n",
        "\\end{equation}\n",
        "\n",
        "Para *corpus* grandes e com exemplos contendo muitas palavras, o cálculo acima é inviável. Por isso, utilizamos a suposição de Andrei Markov em que pegamos apenas os $k-1$ termos já processados:\n",
        "\n",
        "\\begin{equation}\n",
        "P(w_i | w_1, w_2, \\ldots,w_{i-1}) \\approx P(w_i|w_1, \\ldots , w_{i-k})\n",
        "\\end{equation}\n",
        "\n",
        "em que se $k=1$, temos a aproximação para o modelo Unigrama:\n",
        "\n",
        "\\begin{equation}\n",
        "P(w_i | w_1, w_2, \\ldots,w_{i-1}) \\approx P(w_i)\n",
        "\\end{equation}\n",
        "\n",
        "e, por sua vez, se $k=2$, temos a aproximação para o modelo Bigrama:\n",
        "\n",
        "\\begin{equation}\n",
        "P(w_i | w_1, w_2, \\ldots,w_{i-1}) \\approx P(w_i| w_{i-1})\n",
        "\\end{equation}\n",
        "\n",
        "### Exemplo\n",
        "\n",
        "Se $k=2$, para calcularmos a probabilidade de obtermos a frase \"melhor coisa da vida\" no modelo de linguagem Bigrama, devemos calcular o seguinte produtório:\n",
        "\n",
        "$P(``\\textrm{melhor coisa da vida''}) = P(``\\textrm{melhor''},``\\textrm{coisa''},``\\textrm{da''},``\\textrm{vida''}) = $\n",
        "\n",
        "$= P(``\\textrm{melhor''}) \\times P(``\\textrm{coisa''}|``\\textrm{melhor''}) \\times P(``\\textrm{da''}|``\\textrm{melhor''}) \\times P(``\\textrm{vida''}|``\\textrm{da''})$\n",
        "\n",
        "### Como calcular as probabilidades?\n",
        "\n",
        "A principal informação que temos em mãos é a contagem de termos no *corpus*, o que depende do tipo de N-grama utilizado.\n",
        "\n",
        "No Unigrama, as probabilidades são calculadas como a frequência de cada palavra normalizada pela quantidade total de palavras do vocabulário:\n",
        "\n",
        "\\begin{equation}\n",
        "P(w_i) = \\frac{count(w_i)}{N}\n",
        "\\end{equation}\n",
        "\n",
        "Já no bigrama, precisamos contar a quantidade de ocorrências de cada par de palavras (bigrama) $(w_{i},w_{i-1})$:\n",
        "\n",
        "\\begin{equation}\n",
        "P(w_i|w_{i-1}) = \\frac{count(w_i,w_{i-1})}{count(w_i)}\n",
        "\\end{equation}\n",
        "\n",
        "Vamos representar as funções *count* pelos seguintes histogramas de unigramas e de bigramas. O código-fonte para calcular esses histogramas é fornecido abaixo:"
      ],
      "metadata": {
        "id": "B_srWBL6ZQd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histograma_unigram = {}\n",
        "histograma_bigram = {}"
      ],
      "metadata": {
        "id": "KqT2R1YkA81S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in bigram:\n",
        "    for par in bigram[doc]:\n",
        "        if par in histograma_bigram:\n",
        "            histograma_bigram[par] += bigram[doc][par]\n",
        "        else:\n",
        "            histograma_bigram[par] = bigram[doc][par]"
      ],
      "metadata": {
        "id": "s-rAxgokGU0D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histograma_bigram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8UTi6ZSHbdw",
        "outputId": "b89e3f24-61c9-4b3d-cbec-7dd185d89d1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('A', 'experiência'): 1,\n",
              " ('A', 'pior'): 1,\n",
              " ('Acai', 'é'): 1,\n",
              " ('Batatinha', 'quando'): 1,\n",
              " ('Quero', 'meu'): 1,\n",
              " ('a', 'melhor'): 1,\n",
              " ('chão', 'igual'): 1,\n",
              " ('coisa', 'da'): 1,\n",
              " ('da', 'minha'): 1,\n",
              " ('da', 'vida'): 1,\n",
              " ('de', 'volta'): 1,\n",
              " ('dinheiro', 'de'): 1,\n",
              " ('dinheiro', 'esparrama'): 1,\n",
              " ('do', 'dinheiro'): 1,\n",
              " ('e', 'é'): 1,\n",
              " ('esparrama', 'minha'): 1,\n",
              " ('esparrama', 'pelo'): 1,\n",
              " ('experiência', 'da'): 1,\n",
              " ('experiência', 'do'): 1,\n",
              " ('igual', 'batatinha'): 1,\n",
              " ('melhor', 'coisa'): 1,\n",
              " ('meu', 'dinheiro'): 1,\n",
              " ('meu', 'e'): 1,\n",
              " ('minha', 'vida'): 2,\n",
              " ('nasce', 'esparrama'): 1,\n",
              " ('pelo', 'chão'): 1,\n",
              " ('pior', 'experiência'): 1,\n",
              " ('pois', 'é'): 1,\n",
              " ('quando', 'nasce'): 1,\n",
              " ('volta', 'pois'): 1,\n",
              " ('é', 'a'): 1,\n",
              " ('é', 'meu'): 2}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in unigram:\n",
        "    for par in unigram[doc]:\n",
        "        if par in histograma_unigram:\n",
        "            histograma_unigram[par] += unigram[doc][par]\n",
        "        else:\n",
        "            histograma_unigram[par] = unigram[doc][par]"
      ],
      "metadata": {
        "id": "aU6f-PqkHThN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histograma_unigram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCND2lUIHhmL",
        "outputId": "74058032-69dd-4ca3-c1ab-7a2d15cbae76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': 2,\n",
              " 'Acai': 1,\n",
              " 'Batatinha': 1,\n",
              " 'Quero': 1,\n",
              " 'a': 1,\n",
              " 'batatinha': 1,\n",
              " 'chão': 1,\n",
              " 'coisa': 1,\n",
              " 'da': 2,\n",
              " 'de': 1,\n",
              " 'dinheiro': 2,\n",
              " 'do': 1,\n",
              " 'e': 1,\n",
              " 'esparrama': 2,\n",
              " 'experiência': 2,\n",
              " 'igual': 1,\n",
              " 'melhor': 1,\n",
              " 'meu': 3,\n",
              " 'minha': 2,\n",
              " 'nasce': 1,\n",
              " 'pelo': 1,\n",
              " 'pior': 1,\n",
              " 'pois': 1,\n",
              " 'quando': 1,\n",
              " 'vida': 3,\n",
              " 'volta': 1,\n",
              " 'é': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos criar uma função que calcula a probabilidade de obtermos uma determinada sentença qualquer de \"teste\".\n",
        "\n",
        "**Não finalizado! Falta tratar o log no produtório e casos de probabilidade zero. Continuação na aula do dia 07/07/2022**"
      ],
      "metadata": {
        "id": "tCdOZEkxlPAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(sentence):\n",
        "\n",
        "    tokens = sentence.split()\n",
        "\n",
        "    p = histograma_unigram[tokens[0]]/n\n",
        "\n",
        "    for i in range(0,len(tokens)-1):\n",
        "        p = p*(histograma_bigram[(tokens[i],tokens[i+1])]/histograma_unigram[tokens[i+1]])\n",
        "    \n",
        "    return p"
      ],
      "metadata": {
        "id": "tQEsfE82Hv0k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentença de teste"
      ],
      "metadata": {
        "id": "MFBVVaRvH1iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob(\"Batatinha\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI-OOXoUH2pR",
        "outputId": "5d5ebfc7-c490-44a4-9aef-d7be6fd9737c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02564102564102564"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}
